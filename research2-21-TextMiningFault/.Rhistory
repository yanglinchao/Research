setwd("C:/Users/ylc/GitHub/Research/research2-21-TextMiningFault")
library(jiebaR)
### 载入数据
tf_data <- read.csv("tf.csv")
### 分词
## 建立分词器
# 建立分词器,使用了mix建模,百度、哈工大、川大合并的停止词表
worker <- worker(type = "mix",
bylines = TRUE,
stop_word = "C:/Users/ylc/GitHub/Research/research2-21-TextMiningFault/stopwords/bd_hit_scu_stopwords.txt",
encoding = "UTF-8")
## 对故障现象进行分词
# 提取数据
data_appearance <- tf_data[, c("编号", "故障现象")]
# 提取故障现象向量
vec_appearance <- data_appearance$故障现象
# 分词，cut_appearance是分词结果！！！！
cut_appearance <- segment(vec_appearance, worker)
# 生成一个结果数据框
cutdata_appearance <- data.frame(id = rep(data_appearance$编号, each=2), appearance = rep(data_appearance$故障现象, each=2))
setwd("C:/Users/ylc/GitHub/Research/research2-21-TextMiningFault")
setwd("~/GitHub/Research/research2-21-TextMiningFault")
