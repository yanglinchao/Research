setwd("C:/Users/ylc/GitHub/Research/research2-21-TextMiningFault")

library(jiebaR)


### 载入数据
tf_data <- read.csv("tf.csv")

### 分词

## 建立分词器
# 建立分词器,使用了mix建模,百度、哈工大、川大合并的停止词表
worker <- worker(type = "mix",
                 bylines = TRUE,
                 stop_word = "C:/Users/ylc/GitHub/Research/research2-21-TextMiningFault/stopwords/bd_hit_scu_stopwords.txt",
                 encoding = "UTF-8")

## 对故障现象进行分词
# 提取数据
data_appearance <- tf_data[, c("编号", "故障现象")]
# 提取故障现象向量
vec_appearance <- data_appearance$故障现象
# 分词，cut_appearance是分词结果！！！！
cut_appearance <- segment(vec_appearance, worker)
# 生成一个结果数据框
cutdata_appearance <- data.frame(id = rep(data_appearance$编号, each=2), appearance = rep(data_appearance$故障现象, each=2))
# 将分词结果整理
for(i in 1:(nrow(cutdata_appearance)/2)){
  cutdata_appearance$appearance[2*i] <- paste(cut_appearance[[i]], collapse = " ")
}
# 输出结果
write.csv(cutdata_appearance, "cutdata_appearance.csv", row.names = FALSE)


## 对处理方式进行分词
# 提取数据
data_solve <- tf_data[, c("编号", "处理方式")]
# 将数据整理为向量
vec_solve <- data_solve$处理方式
# 分词，cut_solve是分词结果！！！！！
cut_solve <- segment(vec_solve, worker)
# 生成一个结果数据框
cutdata_solve <- data.frame(id = rep(data_solve$编号, each=2), solve = rep(data_solve$处理方式, each=2))
# 将分词结果整理
for(i in 1:(nrow(cutdata_solve)/2)){
  cutdata_solve$solve[2*i] <- paste(cut_solve[[i]], collapse = " ")
}
# 输出分词结果
write.csv(cutdata_solve, "cutdata_solve.csv", row.names = FALSE)
